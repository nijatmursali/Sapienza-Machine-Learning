{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras \n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    "                         Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\\\n",
    "                         UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import applications\n",
    "from keras.models import Model, Input\n",
    "\n",
    "print(\"Tensorflow version %s\" %tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainingset = \"C:/HW/IMAGES/raining/\"\n",
    "testset = \"C:/HW/IMAGES/est/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 4 classes.\n",
      "Found 80 images belonging to 4 classes.\n",
      "Image input (118, 224, 3)\n",
      "Classes: ['HAZE', 'RAINY', 'SNOWY', 'SUNNY']\n",
      "Loaded 320 training samples from 4 classes.\n",
      "Loaded 80 test samples from 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "input_shape = ()\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255,\\\n",
    "    zoom_range=0.1,\\\n",
    "    rotation_range=10,\\\n",
    "    width_shift_range=0.1,\\\n",
    "    height_shift_range=0.1,\\\n",
    "    horizontal_flip=True,\\\n",
    "    vertical_flip=False)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=trainingset,\n",
    "    target_size=(118, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=testset,\n",
    "    target_size=(118, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_samples = train_generator.n\n",
    "num_classes = train_generator.num_classes\n",
    "input_shape = train_generator.image_shape\n",
    "\n",
    "classnames = [k for k,v in train_generator.class_indices.items()]\n",
    "\n",
    "print(\"Image input %s\" %str(input_shape))\n",
    "print(\"Classes: %r\" %classnames)\n",
    "\n",
    "print('Loaded %d training samples from %d classes.' %(num_samples,num_classes))\n",
    "print('Loaded %d test samples from %d classes.' %(test_generator.n,test_generator.num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 3\n",
    "x,y = train_generator.next()\n",
    "# x,y size is train_generator.batch_size\n",
    "\n",
    "for i in range(0,n):\n",
    "    image = x[i]\n",
    "    label = y[i].argmax()  # categorical from one-hot-encoding\n",
    "    print(classnames[label])\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nijat\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"RandNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 117, 223, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 116, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 116, 222, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 115, 221, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 114, 220, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 114, 220, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 113, 219, 165)     42405     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 112, 218, 165)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 112, 218, 165)     660       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4028640)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               402864100 \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 402,917,025\n",
      "Trainable params: 402,916,303\n",
      "Non-trainable params: 722\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def RandNet(input_shape, num_classes):\n",
    "    \n",
    "    model = Sequential(name=\"RandNet\")\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), strides=(1, 1), activation='relu', input_shape=input_shape, padding=\"valid\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(2, 2), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(165, kernel_size=(2, 2), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    optimizer = 'adam' #alternative 'SGD'\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    " \n",
    "# create the model\n",
    "model = RandNet(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nijat\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nijat\\AppData\\Roaming\\Python\\Python36\\site-packages\\PIL\\TiffImagePlugin.py:780: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1676s 335s/step - loss: 1.6030 - accuracy: 0.6031 - val_loss: 0.5909 - val_accuracy: 0.3125\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nijat\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/5 [=====>........................] - ETA: 22:56 - loss: 1.5544 - accuracy: 0.6875"
     ]
    }
   ],
   "source": [
    "stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=4)\n",
    "\n",
    "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
    "val_steps=test_generator.n//test_generator.batch_size+1\n",
    "\n",
    "try:\n",
    "    history = model.fit_generator(train_generator, epochs=20, verbose=1, callbacks=[stopping],\\\n",
    "                    steps_per_epoch=steps_per_epoch,\\\n",
    "                    validation_data=test_generator,\\\n",
    "                    validation_steps=val_steps)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_backbone_net(input_shape):\n",
    "    \n",
    "    # define input tensor\n",
    "    input0 = Input(shape=input_shape)\n",
    "\n",
    "    # load a pretrained model on imagenet without the final dense layer\n",
    "    feature_extractor = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=input0)\n",
    "    \n",
    "    \n",
    "    feature_extractor = feature_extractor.output\n",
    "    feature_extractor = Model(input=input0, output=feature_extractor)\n",
    "    optimizer = 'adam' #alternative 'SGD'\n",
    "\n",
    "    feature_extractor.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return feature_extractor\n",
    "\n",
    "\n",
    "def transferNet(feature_extractor, num_classes, output_layer_name, trainable_layers):\n",
    "    \n",
    "    # get the original input layer tensor\n",
    "    input_t = feature_extractor.get_layer(index=0).input\n",
    "\n",
    "    # set the feture extractor layers as non-trainable\n",
    "    for idx,layer in enumerate(feature_extractor.layers):\n",
    "      if layer.name in trainable_layers:\n",
    "        layer.trainable = True\n",
    "      else:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # get the output tensor from a layer of the feature extractor\n",
    "    output_extractor = feature_extractor.get_layer(name = output_layer_name).output\n",
    "    \n",
    "    #output_extractor = MaxPooling2D(pool_size=(4,4))(output_extractor)\n",
    "\n",
    "    # flat the output of a Conv layer\n",
    "    flatten = Flatten()(output_extractor) \n",
    "    flatten_norm = BatchNormalization()(flatten)\n",
    "\n",
    "    # add a Dense layer\n",
    "    dense = Dropout(0.4)(flatten_norm)\n",
    "    dense = Dense(200, activation='relu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    \n",
    "    # add a Dense layer\n",
    "    dense = Dropout(0.4)(dense)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "\n",
    "    # add the final output layer\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dense(num_classes, activation='softmax')(dense)\n",
    "    \n",
    "\n",
    "    model = Model(input=input_t, output=dense, name=\"transferNet\")\n",
    "    \n",
    "    optimizer = 'adam' #alternative 'SGD'\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# load the pre-trained model\n",
    "feature_extractor = load_backbone_net(input_shape)\n",
    "feature_extractor.summary()\n",
    "\n",
    "\n",
    "# choose the layer from which you can get the features (block5_pool the end, glob_pooling to get the pooled version of the output)\n",
    "name_output_extractor = \"block5_pool\"\n",
    "trainable_layers = [\"block5_conv3\"]\n",
    "\n",
    "# build the transfer model\n",
    "transfer_model = transferNet(feature_extractor, num_classes, name_output_extractor, trainable_layers)\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the transferNet on the training data\n",
    "stopping = callbacks.EarlyStopping(monitor='val_acc', patience=3)\n",
    "\n",
    "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
    "val_steps = test_generator.n//test_generator.batch_size+1\n",
    "\n",
    "try:\n",
    "    history_transfer = transfer_model.fit_generator(train_generator, epochs=100, verbose=1, callbacks=[stopping],\\\n",
    "                    steps_per_epoch=steps_per_epoch,\\\n",
    "                    validation_data=test_generator,\\\n",
    "                    validation_steps=val_steps)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the data generator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255,\\\n",
    "    zoom_range=0.1,\\\n",
    "    rotation_range=1,\\\n",
    "    width_shift_range=0,\\\n",
    "    height_shift_range=0,\\\n",
    "    horizontal_flip=True,\\\n",
    "    vertical_flip=False)\n",
    "\n",
    "\n",
    "# set the shuffle flag to False\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=trainingset,\n",
    "    target_size=(118, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a shallow neural net\n",
    "def shallowNet(feature_extractor, num_classes):\n",
    "    \n",
    "    # get the original input layer tensor\n",
    "    input_t = feature_extractor.get_layer(index=0).input\n",
    "\n",
    "    # set the feture extractor layers as non-trainable\n",
    "    for idx,layer in enumerate(feature_extractor.layers):\n",
    "        layer.trainable = False\n",
    "\n",
    "    # get the last tensor from the feature extractor\n",
    "    output_extractor = feature_extractor.output\n",
    "    \n",
    "    flatten = Flatten()(output_extractor) \n",
    "    flatten_norm = BatchNormalization()(flatten)\n",
    "    \n",
    "    # add the final output layer\n",
    "    dense = Dense(num_classes, activation='softmax')(flatten_norm)\n",
    "    \n",
    "\n",
    "    shallow_model = Model(input=input_t, output=dense, name=\"shallowNet\")\n",
    "    \n",
    "    \n",
    "    optimizer = 'adam' #alternative 'SGD'\n",
    "    shallow_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return shallow_model\n",
    "\n",
    "\n",
    "shallow_model = shallowNet(feature_extractor, num_classes)\n",
    "shallow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the shallow model\n",
    "try:\n",
    "    history_shallow= shallow_model.fit_generator(train_generator, epochs=5, verbose=1, callbacks=[stopping],\\\n",
    "                    steps_per_epoch=steps_per_epoch,\\\n",
    "                    validation_data=test_generator,\\\n",
    "                    validation_steps=val_steps)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "val_steps=test_generator.n//test_generator.batch_size+1\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=testset,\n",
    "    target_size=(118, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# predictions from transferNet\n",
    "preds = transfer_model.predict_generator(test_generator,verbose=1,steps=val_steps)\n",
    "\n",
    "Ypred = np.argmax(preds, axis=1)\n",
    "Ytest = test_generator.classes  # shuffle=False in test_generator\n",
    "\n",
    "cm = confusion_matrix(Ytest, Ypred)\n",
    "\n",
    "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
    "for i in range(0,cm.shape[0]):\n",
    "  for j in range(0,cm.shape[1]):\n",
    "    if (i!=j and cm[i][j]>0):\n",
    "      conf.append([i,j,cm[i][j]])\n",
    "\n",
    "col=2\n",
    "conf = np.array(conf)\n",
    "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
    "\n",
    "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
    "print('------------------------------------------------------------------')\n",
    "for k in conf:\n",
    "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history,name):\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title(name + ' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(name + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "name=\"RandNet\"\n",
    "plot_history(history, name)\n",
    "name=\"transferNet\"\n",
    "plot_history(history_transfer, name)\n",
    "name=\"shallowNet\"\n",
    "plot_history(history_shallow, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
